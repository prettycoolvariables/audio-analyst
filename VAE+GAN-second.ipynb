{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841c409f-015a-4d61-abda-fa4f73ee00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import librosa\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c6c496-dc77-47e6-8275-efe458c698d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(directory):\n",
    "    data, labels = [], []\n",
    "    for label, folder in enumerate(os.listdir(directory)):\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if not os.path.isdir(folder_path):  # Skip files, only process directories\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            if not file_path.endswith(('.wav', '.mp3')):  # Add valid audio file extensions\n",
    "                continue\n",
    "            signal, sr = librosa.load(file_path, sr=22050)\n",
    "            \n",
    "            # Generate Mel spectrogram with 128 Mel bands\n",
    "            mel_spec = librosa.feature.melspectrogram(y=signal, sr=sr, n_mels=128)\n",
    "            mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Resize to (128, 128)\n",
    "            mel_spec_resized = cv2.resize(mel_spec, (128, 128))\n",
    "\n",
    "            # Expand dimensions to add channel (for CNN input compatibility)\n",
    "            data.append(mel_spec_resized)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Convert to numpy arrays and add channel dimension\n",
    "    data = np.array(data)\n",
    "    data = np.expand_dims(data, -1)  # Shape will be (num_samples, 128, 128, 1)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23f96d0-a35d-4dec-ad3b-f4a04a36fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize data for VAE-GAN input\n",
    "def resize_data(data, target_shape=(32, 32)):\n",
    "    resized_data = [cv2.resize(img, target_shape) for img in data]\n",
    "    resized_data = np.expand_dims(np.array(resized_data), -1)\n",
    "    return resized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e2a0db-1864-4feb-869a-7179a705af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data, labels = load_audio_files('C:/Users/HP/Downloads/archive/Raw Audio/')\n",
    "data_resized = resize_data(data, target_shape=(32, 32))\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(data_resized, labels, test_size=0.4, random_state=42)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(temp_data, temp_labels, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ffd9c52-e70e-411f-a0d5-c71fa75b0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE-GAN components and training process\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_encoder(latent_dim):\n",
    "    inputs = keras.Input(shape=(32, 32, 1))\n",
    "    x = layers.Conv2D(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.Conv2D(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    return keras.Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "def build_decoder(latent_dim):\n",
    "    inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(8 * 8 * 64, activation=\"relu\")(inputs)\n",
    "    x = layers.Reshape((8, 8, 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    outputs = layers.Conv2DTranspose(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    return keras.Model(inputs, outputs, name=\"decoder\")\n",
    "\n",
    "def build_discriminator():\n",
    "    inputs = keras.Input(shape=(32, 32, 1))\n",
    "    x = layers.Conv2D(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.Conv2D(128, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    return keras.Model(inputs, outputs, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "502186de-b068-46b8-bb1b-caa26ab075b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 44ms/step - disc_loss: 0.1766 - gen_loss: 7.6216 - vae_loss: 52982.6445 - val_loss: 0.0000e+00\n",
      "Epoch 2/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - disc_loss: 3.8727e-09 - gen_loss: 21.1377 - vae_loss: 2689.4346 - val_loss: 0.0000e+00\n",
      "Epoch 3/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - disc_loss: 9.9097e-10 - gen_loss: 22.9103 - vae_loss: 2685.6521 - val_loss: 0.0000e+00\n",
      "Epoch 4/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - disc_loss: 8.0516e-10 - gen_loss: 22.9943 - vae_loss: 2694.6091 - val_loss: 0.0000e+00\n",
      "Epoch 5/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 6.7972e-10 - gen_loss: 22.9974 - vae_loss: 2684.3838 - val_loss: 0.0000e+00\n",
      "Epoch 6/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - disc_loss: 6.0126e-10 - gen_loss: 22.9971 - vae_loss: 2695.6396 - val_loss: 0.0000e+00\n",
      "Epoch 7/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - disc_loss: 5.1380e-10 - gen_loss: 22.9968 - vae_loss: 2663.5686 - val_loss: 0.0000e+00\n",
      "Epoch 8/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - disc_loss: 4.6186e-10 - gen_loss: 22.9966 - vae_loss: 2701.4968 - val_loss: 0.0000e+00\n",
      "Epoch 9/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - disc_loss: 4.1811e-10 - gen_loss: 22.9964 - vae_loss: 2707.0498 - val_loss: 0.0000e+00\n",
      "Epoch 10/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - disc_loss: 3.8035e-10 - gen_loss: 22.9963 - vae_loss: 2726.1858 - val_loss: 0.0000e+00\n",
      "Epoch 11/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - disc_loss: 3.9693e-10 - gen_loss: 22.9962 - vae_loss: 2684.3716 - val_loss: 0.0000e+00\n",
      "Epoch 12/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 3.3064e-10 - gen_loss: 22.9961 - vae_loss: 2704.1140 - val_loss: 0.0000e+00\n",
      "Epoch 13/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - disc_loss: 3.1119e-10 - gen_loss: 22.9961 - vae_loss: 2699.0264 - val_loss: 0.0000e+00\n",
      "Epoch 14/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - disc_loss: 2.9418e-10 - gen_loss: 22.9960 - vae_loss: 2679.1692 - val_loss: 0.0000e+00\n",
      "Epoch 15/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 2.8102e-10 - gen_loss: 22.9959 - vae_loss: 2720.8210 - val_loss: 0.0000e+00\n",
      "Epoch 16/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - disc_loss: 2.6822e-10 - gen_loss: 22.9959 - vae_loss: 2707.5903 - val_loss: 0.0000e+00\n",
      "Epoch 17/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - disc_loss: 2.5560e-10 - gen_loss: 22.9958 - vae_loss: 2682.3669 - val_loss: 0.0000e+00\n",
      "Epoch 18/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 2.4486e-10 - gen_loss: 22.9958 - vae_loss: 2673.3223 - val_loss: 0.0000e+00\n",
      "Epoch 19/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - disc_loss: 2.4249e-10 - gen_loss: 22.9957 - vae_loss: 2694.9172 - val_loss: 0.0000e+00\n",
      "Epoch 20/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - disc_loss: 4.7874e-10 - gen_loss: 22.9956 - vae_loss: 2669.3523 - val_loss: 0.0000e+00\n",
      "Epoch 21/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 2.1917e-10 - gen_loss: 22.9956 - vae_loss: 2703.6719 - val_loss: 0.0000e+00\n",
      "Epoch 22/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 2.1369e-10 - gen_loss: 22.9955 - vae_loss: 2695.4373 - val_loss: 0.0000e+00\n",
      "Epoch 23/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - disc_loss: 2.0803e-10 - gen_loss: 22.9955 - vae_loss: 2699.1816 - val_loss: 0.0000e+00\n",
      "Epoch 24/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 2.1818e-10 - gen_loss: 22.9955 - vae_loss: 2670.1572 - val_loss: 0.0000e+00\n",
      "Epoch 25/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - disc_loss: 2.0064e-10 - gen_loss: 22.9954 - vae_loss: 2709.1931 - val_loss: 0.0000e+00\n",
      "Epoch 26/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 2.1829e-10 - gen_loss: 22.9954 - vae_loss: 2659.2112 - val_loss: 0.0000e+00\n",
      "Epoch 27/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 1.9507e-10 - gen_loss: 22.9954 - vae_loss: 2699.2217 - val_loss: 0.0000e+00\n",
      "Epoch 28/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 1.9176e-10 - gen_loss: 22.9954 - vae_loss: 2707.8550 - val_loss: 0.0000e+00\n",
      "Epoch 29/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 1.8890e-10 - gen_loss: 22.9953 - vae_loss: 2687.4717 - val_loss: 0.0000e+00\n",
      "Epoch 30/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - disc_loss: 1.8532e-10 - gen_loss: 22.9953 - vae_loss: 2712.6780 - val_loss: 0.0000e+00\n",
      "Epoch 31/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - disc_loss: 1.8252e-10 - gen_loss: 22.9953 - vae_loss: 2659.1953 - val_loss: 0.0000e+00\n",
      "Epoch 32/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - disc_loss: 1.8003e-10 - gen_loss: 22.9953 - vae_loss: 2675.9414 - val_loss: 0.0000e+00\n",
      "Epoch 33/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - disc_loss: 1.7747e-10 - gen_loss: 22.9952 - vae_loss: 2717.6975 - val_loss: 0.0000e+00\n",
      "Epoch 34/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - disc_loss: 1.7478e-10 - gen_loss: 22.9952 - vae_loss: 2703.2629 - val_loss: 0.0000e+00\n",
      "Epoch 35/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - disc_loss: 1.7164e-10 - gen_loss: 22.9952 - vae_loss: 2663.2458 - val_loss: 0.0000e+00\n",
      "Epoch 36/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - disc_loss: 1.6889e-10 - gen_loss: 22.9951 - vae_loss: 2696.9998 - val_loss: 0.0000e+00\n",
      "Epoch 37/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - disc_loss: 1.6617e-10 - gen_loss: 22.9951 - vae_loss: 2699.0752 - val_loss: 0.0000e+00\n",
      "Epoch 38/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - disc_loss: 1.9511e-10 - gen_loss: 22.9951 - vae_loss: 2691.1536 - val_loss: 0.0000e+00\n",
      "Epoch 39/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - disc_loss: 1.6123e-10 - gen_loss: 22.9951 - vae_loss: 2692.5195 - val_loss: 0.0000e+00\n",
      "Epoch 40/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - disc_loss: 1.5927e-10 - gen_loss: 22.9951 - vae_loss: 2699.2087 - val_loss: 0.0000e+00\n",
      "Epoch 41/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - disc_loss: 1.5769e-10 - gen_loss: 22.9950 - vae_loss: 2681.3777 - val_loss: 0.0000e+00\n",
      "Epoch 42/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - disc_loss: 1.6853e-10 - gen_loss: 22.9950 - vae_loss: 2665.0107 - val_loss: 0.0000e+00\n",
      "Epoch 43/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - disc_loss: 1.5411e-10 - gen_loss: 22.9950 - vae_loss: 2668.1919 - val_loss: 0.0000e+00\n",
      "Epoch 44/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - disc_loss: 1.8242e-10 - gen_loss: 22.9950 - vae_loss: 2683.0657 - val_loss: 0.0000e+00\n",
      "Epoch 45/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - disc_loss: 1.5047e-10 - gen_loss: 22.9950 - vae_loss: 2662.8291 - val_loss: 0.0000e+00\n",
      "Epoch 46/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - disc_loss: 1.4885e-10 - gen_loss: 22.9949 - vae_loss: 2691.6772 - val_loss: 0.0000e+00\n",
      "Epoch 47/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - disc_loss: 1.4717e-10 - gen_loss: 22.9949 - vae_loss: 2708.6228 - val_loss: 0.0000e+00\n",
      "Epoch 48/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - disc_loss: 1.4562e-10 - gen_loss: 22.9949 - vae_loss: 2700.8813 - val_loss: 0.0000e+00\n",
      "Epoch 49/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - disc_loss: 1.4387e-10 - gen_loss: 22.9949 - vae_loss: 2688.8125 - val_loss: 0.0000e+00\n",
      "Epoch 50/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - disc_loss: 1.4226e-10 - gen_loss: 22.9949 - vae_loss: 2708.1963 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fb09af5b10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VAE_GAN(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, discriminator):\n",
    "        super(VAE_GAN, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def compile(self, vae_optimizer, disc_optimizer, gen_optimizer, **kwargs):\n",
    "        super(VAE_GAN, self).compile(**kwargs)\n",
    "        self.vae_optimizer = vae_optimizer\n",
    "        self.disc_optimizer = disc_optimizer\n",
    "        self.gen_optimizer = gen_optimizer\n",
    "        self.reconstruction_loss_fn = keras.losses.MeanSquaredError()\n",
    "        self.gan_loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Handle tuple input (inputs, labels) or standalone inputs\n",
    "        if isinstance(data, tuple):\n",
    "            inputs, _ = data\n",
    "        else:\n",
    "            inputs = data\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # VAE forward pass\n",
    "            z_mean, z_log_var, z = self.encoder(inputs)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            # VAE Loss\n",
    "            reconstruction_loss = tf.reduce_mean(self.reconstruction_loss_fn(inputs, reconstruction))\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            vae_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "            # Discriminator Loss\n",
    "            real_labels = tf.ones((tf.shape(inputs)[0], 1))\n",
    "            fake_labels = tf.zeros((tf.shape(inputs)[0], 1))\n",
    "\n",
    "            disc_loss_real = self.gan_loss_fn(real_labels, self.discriminator(inputs))\n",
    "            disc_loss_fake = self.gan_loss_fn(fake_labels, self.discriminator(reconstruction))\n",
    "            disc_loss = (disc_loss_real + disc_loss_fake) / 2\n",
    "\n",
    "            # Generator Loss\n",
    "            gen_loss = self.gan_loss_fn(real_labels, self.discriminator(reconstruction))\n",
    "\n",
    "        # Compute Gradients\n",
    "        vae_gradients = tape.gradient(vae_loss, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
    "        disc_gradients = tape.gradient(disc_loss, self.discriminator.trainable_weights)\n",
    "        gen_gradients = tape.gradient(gen_loss, self.decoder.trainable_weights)\n",
    "\n",
    "        # Apply Gradients\n",
    "        self.vae_optimizer.apply_gradients(zip(vae_gradients, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
    "        self.disc_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_weights))\n",
    "        self.gen_optimizer.apply_gradients(zip(gen_gradients, self.decoder.trainable_weights))\n",
    "\n",
    "        # Return loss values for logging\n",
    "        return {\"vae_loss\": vae_loss, \"disc_loss\": disc_loss, \"gen_loss\": gen_loss}\n",
    "\n",
    "# Define model components\n",
    "latent_dim = 16\n",
    "encoder = build_encoder(latent_dim)  # Replace with your actual encoder\n",
    "decoder = build_decoder(latent_dim)  # Replace with your actual decoder\n",
    "discriminator = build_discriminator()  # Replace with your actual discriminator\n",
    "\n",
    "# Create and compile the VAE-GAN model\n",
    "vae_gan = VAE_GAN(encoder, decoder, discriminator)\n",
    "vae_gan.compile(\n",
    "    vae_optimizer=keras.optimizers.Adam(),\n",
    "    disc_optimizer=keras.optimizers.Adam(),\n",
    "    gen_optimizer=keras.optimizers.Adam(),\n",
    "    loss=lambda y_true, y_pred: 0.0  # Dummy loss to satisfy compile() requirements\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "vae_gan.fit(\n",
    "    train_data,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(val_data, val_data)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "044ac0e3-d366-4bdc-bc08-dc36cc75bb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Evaluation:\n",
      "  Accuracy: 0.8452\n",
      "  Precision: 0.1176\n",
      "  Recall: 0.4615\n",
      "  F1 Score: 0.1875\n",
      "  Confusion Matrix:\n",
      "[[278  45]\n",
      " [  7   6]]\n",
      "\n",
      "Testing Set Evaluation:\n",
      "  Accuracy: 0.8185\n",
      "  Precision: 0.0588\n",
      "  Recall: 0.1875\n",
      "  F1 Score: 0.0896\n",
      "  Confusion Matrix:\n",
      "[[272  48]\n",
      " [ 13   3]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate reconstruction errors\n",
    "def calculate_reconstruction_error(data, model):\n",
    "    z_mean, z_log_var, z = model.encoder(data)\n",
    "    reconstruction = model.decoder(z)\n",
    "    return tf.reduce_mean(tf.square(data - reconstruction), axis=[1, 2, 3]).numpy()\n",
    "\n",
    "# Evaluate performance\n",
    "for dataset_name, dataset_data, dataset_labels in [\n",
    "    (\"Validation\", val_data, val_labels),\n",
    "    (\"Testing\", test_data, test_labels)\n",
    "]:\n",
    "    errors = calculate_reconstruction_error(dataset_data, vae_gan)\n",
    "    threshold = np.percentile(errors, 85)\n",
    "    predictions = errors > threshold\n",
    "\n",
    "    binary_labels = (dataset_labels == 1)\n",
    "    accuracy = accuracy_score(binary_labels, predictions)\n",
    "    precision = precision_score(binary_labels, predictions, zero_division=1)\n",
    "    recall = recall_score(binary_labels, predictions, zero_division=1)\n",
    "    f1 = f1_score(binary_labels, predictions, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(binary_labels, predictions)\n",
    "\n",
    "    print(f\"{dataset_name} Set Evaluation:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\\n{conf_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce053116-f02c-4810-a469-d1d9f22dccbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

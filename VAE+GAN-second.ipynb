{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "841c409f-015a-4d61-abda-fa4f73ee00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import librosa\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9c6c496-dc77-47e6-8275-efe458c698d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(directory):\n",
    "    data, labels = [], []\n",
    "    for label, folder in enumerate(os.listdir(directory)):\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if not os.path.isdir(folder_path):  # Skip files, only process directories\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            if not file_path.endswith(('.wav', '.mp3')):  # Add valid audio file extensions\n",
    "                continue\n",
    "            signal, sr = librosa.load(file_path, sr=22050)\n",
    "            \n",
    "            # Generate Mel spectrogram with 128 Mel bands\n",
    "            mel_spec = librosa.feature.melspectrogram(y=signal, sr=sr, n_mels=128)\n",
    "            mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Resize to (128, 128)\n",
    "            mel_spec_resized = cv2.resize(mel_spec, (128, 128))\n",
    "\n",
    "            # Expand dimensions to add channel (for CNN input compatibility)\n",
    "            data.append(mel_spec_resized)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Convert to numpy arrays and add channel dimension\n",
    "    data = np.array(data)\n",
    "    data = np.expand_dims(data, -1)  # Shape will be (num_samples, 128, 128, 1)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f23f96d0-a35d-4dec-ad3b-f4a04a36fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize data for VAE-GAN input\n",
    "def resize_data(data, target_shape=(32, 32)):\n",
    "    resized_data = [cv2.resize(img, target_shape) for img in data]\n",
    "    resized_data = np.expand_dims(np.array(resized_data), -1)\n",
    "    return resized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81e2a0db-1864-4feb-869a-7179a705af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data, labels = load_audio_files('C:/Users/HP/Downloads/archive/Raw Audio/')\n",
    "data_resized = resize_data(data, target_shape=(32, 32))\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(data_resized, labels, test_size=0.4, random_state=42)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(temp_data, temp_labels, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ffd9c52-e70e-411f-a0d5-c71fa75b0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE-GAN components and training process\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_encoder(latent_dim):\n",
    "    inputs = keras.Input(shape=(32, 32, 1))\n",
    "    x = layers.Conv2D(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.Conv2D(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    return keras.Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "def build_decoder(latent_dim):\n",
    "    inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(8 * 8 * 64, activation=\"relu\")(inputs)\n",
    "    x = layers.Reshape((8, 8, 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    outputs = layers.Conv2DTranspose(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    return keras.Model(inputs, outputs, name=\"decoder\")\n",
    "\n",
    "def build_discriminator():\n",
    "    inputs = keras.Input(shape=(32, 32, 1))\n",
    "    x = layers.Conv2D(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.Conv2D(128, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    return keras.Model(inputs, outputs, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "502186de-b068-46b8-bb1b-caa26ab075b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 89ms/step - disc_loss: 0.0508 - gen_loss: 6.7977 - vae_loss: 2693.4761 - val_loss: 0.0000e+00\n",
      "Epoch 2/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - disc_loss: 3.1768e-08 - gen_loss: 17.1507 - vae_loss: 2674.0107 - val_loss: 0.0000e+00\n",
      "Epoch 3/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - disc_loss: 5.8806e-09 - gen_loss: 18.2596 - vae_loss: 2676.4641 - val_loss: 0.0000e+00\n",
      "Epoch 4/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.5791e-09 - gen_loss: 18.3111 - vae_loss: 2693.8945 - val_loss: 0.0000e+00\n",
      "Epoch 5/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.5631e-09 - gen_loss: 18.3140 - vae_loss: 2678.6885 - val_loss: 0.0000e+00\n",
      "Epoch 6/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - disc_loss: 5.5573e-09 - gen_loss: 18.3150 - vae_loss: 2673.0598 - val_loss: 0.0000e+00\n",
      "Epoch 7/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.5520e-09 - gen_loss: 18.3160 - vae_loss: 2678.6677 - val_loss: 0.0000e+00\n",
      "Epoch 8/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.5458e-09 - gen_loss: 18.3171 - vae_loss: 2694.3948 - val_loss: 0.0000e+00\n",
      "Epoch 9/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.5391e-09 - gen_loss: 18.3183 - vae_loss: 2685.0752 - val_loss: 0.0000e+00\n",
      "Epoch 10/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.5316e-09 - gen_loss: 18.3196 - vae_loss: 2675.3435 - val_loss: 0.0000e+00\n",
      "Epoch 11/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.5241e-09 - gen_loss: 18.3210 - vae_loss: 2687.3928 - val_loss: 0.0000e+00\n",
      "Epoch 12/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.5160e-09 - gen_loss: 18.3225 - vae_loss: 2692.8831 - val_loss: 0.0000e+00\n",
      "Epoch 13/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.5076e-09 - gen_loss: 18.3240 - vae_loss: 2711.3796 - val_loss: 0.0000e+00\n",
      "Epoch 14/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.4988e-09 - gen_loss: 18.3256 - vae_loss: 2698.0881 - val_loss: 0.0000e+00\n",
      "Epoch 15/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - disc_loss: 5.4898e-09 - gen_loss: 18.3272 - vae_loss: 2700.1160 - val_loss: 0.0000e+00\n",
      "Epoch 16/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.4804e-09 - gen_loss: 18.3289 - vae_loss: 2697.2168 - val_loss: 0.0000e+00\n",
      "Epoch 17/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.4707e-09 - gen_loss: 18.3307 - vae_loss: 2693.2141 - val_loss: 0.0000e+00\n",
      "Epoch 18/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.4608e-09 - gen_loss: 18.3325 - vae_loss: 2699.7842 - val_loss: 0.0000e+00\n",
      "Epoch 19/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.4505e-09 - gen_loss: 18.3344 - vae_loss: 2685.1992 - val_loss: 0.0000e+00\n",
      "Epoch 20/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.4401e-09 - gen_loss: 18.3363 - vae_loss: 2671.5715 - val_loss: 0.0000e+00\n",
      "Epoch 21/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - disc_loss: 5.4293e-09 - gen_loss: 18.3383 - vae_loss: 2678.6736 - val_loss: 0.0000e+00\n",
      "Epoch 22/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.4184e-09 - gen_loss: 18.3403 - vae_loss: 2689.3127 - val_loss: 0.0000e+00\n",
      "Epoch 23/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - disc_loss: 5.4072e-09 - gen_loss: 18.3424 - vae_loss: 2717.4880 - val_loss: 0.0000e+00\n",
      "Epoch 24/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - disc_loss: 5.3957e-09 - gen_loss: 18.3445 - vae_loss: 2705.5034 - val_loss: 0.0000e+00\n",
      "Epoch 25/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - disc_loss: 5.3841e-09 - gen_loss: 18.3467 - vae_loss: 2709.3762 - val_loss: 0.0000e+00\n",
      "Epoch 26/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.3723e-09 - gen_loss: 18.3489 - vae_loss: 2695.6719 - val_loss: 0.0000e+00\n",
      "Epoch 27/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.3602e-09 - gen_loss: 18.3511 - vae_loss: 2693.5657 - val_loss: 0.0000e+00\n",
      "Epoch 28/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.3480e-09 - gen_loss: 18.3534 - vae_loss: 2673.9702 - val_loss: 0.0000e+00\n",
      "Epoch 29/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.3356e-09 - gen_loss: 18.3557 - vae_loss: 2672.7322 - val_loss: 0.0000e+00\n",
      "Epoch 30/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - disc_loss: 5.3230e-09 - gen_loss: 18.3581 - vae_loss: 2706.9102 - val_loss: 0.0000e+00\n",
      "Epoch 31/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - disc_loss: 5.3102e-09 - gen_loss: 18.3605 - vae_loss: 2701.0115 - val_loss: 0.0000e+00\n",
      "Epoch 32/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - disc_loss: 5.2972e-09 - gen_loss: 18.3629 - vae_loss: 2696.6069 - val_loss: 0.0000e+00\n",
      "Epoch 33/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.2841e-09 - gen_loss: 18.3654 - vae_loss: 2717.2532 - val_loss: 0.0000e+00\n",
      "Epoch 34/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.2709e-09 - gen_loss: 18.3679 - vae_loss: 2692.9517 - val_loss: 0.0000e+00\n",
      "Epoch 35/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - disc_loss: 5.2574e-09 - gen_loss: 18.3705 - vae_loss: 2706.1670 - val_loss: 0.0000e+00\n",
      "Epoch 36/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - disc_loss: 5.2438e-09 - gen_loss: 18.3731 - vae_loss: 2704.6492 - val_loss: 0.0000e+00\n",
      "Epoch 37/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.2301e-09 - gen_loss: 18.3757 - vae_loss: 2686.8398 - val_loss: 0.0000e+00\n",
      "Epoch 38/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - disc_loss: 5.2162e-09 - gen_loss: 18.3784 - vae_loss: 2703.4749 - val_loss: 0.0000e+00\n",
      "Epoch 39/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.2021e-09 - gen_loss: 18.3811 - vae_loss: 2715.7612 - val_loss: 0.0000e+00\n",
      "Epoch 40/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.1879e-09 - gen_loss: 18.3838 - vae_loss: 2701.1270 - val_loss: 0.0000e+00\n",
      "Epoch 41/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.1736e-09 - gen_loss: 18.3866 - vae_loss: 2698.8101 - val_loss: 0.0000e+00\n",
      "Epoch 42/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.1591e-09 - gen_loss: 18.3893 - vae_loss: 2687.3315 - val_loss: 0.0000e+00\n",
      "Epoch 43/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.1446e-09 - gen_loss: 18.3922 - vae_loss: 2681.8015 - val_loss: 0.0000e+00\n",
      "Epoch 44/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.1298e-09 - gen_loss: 18.3950 - vae_loss: 2729.1172 - val_loss: 0.0000e+00\n",
      "Epoch 45/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.1150e-09 - gen_loss: 18.3979 - vae_loss: 2693.1877 - val_loss: 0.0000e+00\n",
      "Epoch 46/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.1001e-09 - gen_loss: 18.4009 - vae_loss: 2722.3208 - val_loss: 0.0000e+00\n",
      "Epoch 47/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - disc_loss: 5.0850e-09 - gen_loss: 18.4038 - vae_loss: 2680.4089 - val_loss: 0.0000e+00\n",
      "Epoch 48/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.0698e-09 - gen_loss: 18.4068 - vae_loss: 2670.6667 - val_loss: 0.0000e+00\n",
      "Epoch 49/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.0545e-09 - gen_loss: 18.4098 - vae_loss: 2678.2678 - val_loss: 0.0000e+00\n",
      "Epoch 50/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - disc_loss: 5.0391e-09 - gen_loss: 18.4129 - vae_loss: 2716.9568 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14725517010>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VAE_GAN(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, discriminator):\n",
    "        super(VAE_GAN, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def compile(self, vae_optimizer, disc_optimizer, gen_optimizer, **kwargs):\n",
    "        super(VAE_GAN, self).compile(**kwargs)\n",
    "        self.vae_optimizer = vae_optimizer\n",
    "        self.disc_optimizer = disc_optimizer\n",
    "        self.gen_optimizer = gen_optimizer\n",
    "        self.reconstruction_loss_fn = keras.losses.MeanSquaredError()\n",
    "        self.gan_loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Handle tuple input (inputs, labels) or standalone inputs\n",
    "        if isinstance(data, tuple):\n",
    "            inputs, _ = data\n",
    "        else:\n",
    "            inputs = data\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # VAE forward pass\n",
    "            z_mean, z_log_var, z = self.encoder(inputs)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            # VAE Loss\n",
    "            reconstruction_loss = tf.reduce_mean(self.reconstruction_loss_fn(inputs, reconstruction))\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            vae_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "            # Discriminator Loss\n",
    "            real_labels = tf.ones((tf.shape(inputs)[0], 1))\n",
    "            fake_labels = tf.zeros((tf.shape(inputs)[0], 1))\n",
    "\n",
    "            disc_loss_real = self.gan_loss_fn(real_labels, self.discriminator(inputs))\n",
    "            disc_loss_fake = self.gan_loss_fn(fake_labels, self.discriminator(reconstruction))\n",
    "            disc_loss = (disc_loss_real + disc_loss_fake) / 2\n",
    "\n",
    "            # Generator Loss\n",
    "            gen_loss = self.gan_loss_fn(real_labels, self.discriminator(reconstruction))\n",
    "\n",
    "        # Compute Gradients\n",
    "        vae_gradients = tape.gradient(vae_loss, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
    "        disc_gradients = tape.gradient(disc_loss, self.discriminator.trainable_weights)\n",
    "        gen_gradients = tape.gradient(gen_loss, self.decoder.trainable_weights)\n",
    "\n",
    "        # Apply Gradients\n",
    "        self.vae_optimizer.apply_gradients(zip(vae_gradients, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
    "        self.disc_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_weights))\n",
    "        self.gen_optimizer.apply_gradients(zip(gen_gradients, self.decoder.trainable_weights))\n",
    "\n",
    "        # Return loss values for logging\n",
    "        return {\"vae_loss\": vae_loss, \"disc_loss\": disc_loss, \"gen_loss\": gen_loss}\n",
    "\n",
    "# Define model components\n",
    "latent_dim = 16\n",
    "encoder = build_encoder(latent_dim)  # Replace with your actual encoder\n",
    "decoder = build_decoder(latent_dim)  # Replace with your actual decoder\n",
    "discriminator = build_discriminator()  # Replace with your actual discriminator\n",
    "\n",
    "# Create and compile the VAE-GAN model\n",
    "vae_gan = VAE_GAN(encoder, decoder, discriminator)\n",
    "vae_gan.compile(\n",
    "    vae_optimizer=keras.optimizers.Adam(),\n",
    "    disc_optimizer=keras.optimizers.Adam(),\n",
    "    gen_optimizer=keras.optimizers.Adam(),\n",
    "    loss=lambda y_true, y_pred: 0.0  # Dummy loss to satisfy compile() requirements\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "vae_gan.fit(\n",
    "    train_data,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(val_data, val_data)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "044ac0e3-d366-4bdc-bc08-dc36cc75bb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Evaluation:\n",
      "  Accuracy: 0.9107\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1 Score: 0.0000\n",
      "  Confusion Matrix:\n",
      "[[306  17]\n",
      " [ 13   0]]\n",
      "\n",
      "Testing Set Evaluation:\n",
      "  Accuracy: 0.9018\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1 Score: 0.0000\n",
      "  Confusion Matrix:\n",
      "[[303  17]\n",
      " [ 16   0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate reconstruction errors\n",
    "def calculate_reconstruction_error(data, model):\n",
    "    z_mean, z_log_var, z = model.encoder(data)\n",
    "    reconstruction = model.decoder(z)\n",
    "    return tf.reduce_mean(tf.square(data - reconstruction), axis=[1, 2, 3]).numpy()\n",
    "\n",
    "# Evaluate performance\n",
    "for dataset_name, dataset_data, dataset_labels in [\n",
    "    (\"Validation\", val_data, val_labels),\n",
    "    (\"Testing\", test_data, test_labels)\n",
    "]:\n",
    "    errors = calculate_reconstruction_error(dataset_data, vae_gan)\n",
    "    threshold = np.percentile(errors, 95)\n",
    "    predictions = errors > threshold\n",
    "\n",
    "    binary_labels = (dataset_labels == 1)\n",
    "    accuracy = accuracy_score(binary_labels, predictions)\n",
    "    precision = precision_score(binary_labels, predictions, zero_division=1)\n",
    "    recall = recall_score(binary_labels, predictions, zero_division=1)\n",
    "    f1 = f1_score(binary_labels, predictions, zero_division=1)\n",
    "    conf_matrix = confusion_matrix(binary_labels, predictions)\n",
    "\n",
    "    print(f\"{dataset_name} Set Evaluation:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\\n{conf_matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce053116-f02c-4810-a469-d1d9f22dccbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

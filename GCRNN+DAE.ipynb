{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e865c3e-c3fa-4023-8faf-bb47ecf659fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import cv2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, UpSampling2D, GRU, Dense, \n",
    "                                     Flatten, TimeDistributed, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da5f5641-8a6b-4236-9466-092c119a9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "def preprocess_audio(file_path, sr=22050, n_mels=128, fixed_length=128):\n",
    "    signal, _ = librosa.load(file_path, sr=sr, mono=True)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr, n_mels=n_mels)\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    if mel_spectrogram_db.shape[1] < fixed_length:\n",
    "        mel_spectrogram_db = np.pad(mel_spectrogram_db, ((0, 0), (0, fixed_length - mel_spectrogram_db.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mel_spectrogram_db = mel_spectrogram_db[:, :fixed_length]\n",
    "\n",
    "    return mel_spectrogram_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac0b528-3354-4c89-96f7-7d5291d09849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(data_path, fixed_length=128):\n",
    "    dataset = []\n",
    "    for label_dir in os.listdir(data_path):\n",
    "        label_path = os.path.join(data_path, label_dir)\n",
    "        if os.path.isdir(label_path):\n",
    "            for file_name in os.listdir(label_path):\n",
    "                file_path = os.path.join(label_path, file_name)\n",
    "                if file_path.endswith('.wav'):\n",
    "                    spectrogram = preprocess_audio(file_path, fixed_length=fixed_length)\n",
    "                    dataset.append(spectrogram)\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf0e86c-5f53-487d-8347-15101d540f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_data(data, target_shape):\n",
    "    resized_data = np.array([cv2.resize(sample, target_shape[:2]) for sample in data])\n",
    "    return resized_data[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a4de63-542a-4ddd-80ef-ffb3bf272489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_normalize(data):\n",
    "    max_val = np.max(data)\n",
    "    if max_val == 0:\n",
    "        return data\n",
    "    return data / max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b4f6d71-9ef1-4ea3-b790-ca4031ff0b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DAE model\n",
    "def build_dae(input_shape):\n",
    "    input_img = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return Model(input_img, decoded, name=\"DenoisingAutoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8934d79c-dfb4-4c66-9b22-170fdab8d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GCRNN model\n",
    "def build_gcrnn(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same'))(inputs)\n",
    "    x = TimeDistributed(BatchNormalization())(x)\n",
    "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
    "    x = TimeDistributed(Flatten())(x)\n",
    "    x = GRU(64, activation='relu', return_sequences=True, dropout=0.2)(x)\n",
    "    x = GRU(32, activation='relu', dropout=0.2)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs, outputs, name=\"GCRNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b385c75-5c16-41b5-b0d7-cabbf44fa000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_data_path = \"C:/Users/HP/Downloads/archive/Raw Audio/\"\n",
    "test_data_path = \"C:/Users/HP/Desktop/Test Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86aa4bc3-74e9-4cdd-bb1d-ba127166bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "train_data = load_audio_files(train_data_path)\n",
    "train_data = resize_data(train_data, target_shape=(32, 32))\n",
    "train_data = safe_normalize(train_data)\n",
    "train_data = train_data[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f5e958-5ca1-4dd5-8ca4-5be751eb8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a34871a4-e8d4-4970-a1a4-12819ae8b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 83ms/step - loss: 1676.9875 - val_loss: 1682.7087 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 1638.9170 - val_loss: 1681.7561 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 1676.2111 - val_loss: 1681.4937 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 1676.2505 - val_loss: 1681.3605 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 1707.2524 - val_loss: 1681.3127 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - loss: 1691.1111 - val_loss: 1681.2957 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 1725.5681 - val_loss: 1681.2633 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 1681.3904 - val_loss: 1681.2549 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 1690.7523 - val_loss: 1681.2360 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 1728.0363 - val_loss: 1681.2091 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 1709.9801 - val_loss: 1681.2144 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 1708.1355 - val_loss: 1681.2015 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 1687.1378 - val_loss: 1681.1919 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 1683.8071 - val_loss: 1681.1771 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 1724.5232 - val_loss: 1681.1592 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 1728.2126 - val_loss: 1681.1547 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 1677.8052 - val_loss: 1681.1564 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 1714.1331 - val_loss: 1681.1575 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 1673.4153 - val_loss: 1681.1550 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 1694.5627 - val_loss: 1681.1564 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - loss: 1729.7418 - val_loss: 1681.1571 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b225360ed0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train DAE\n",
    "dae = build_dae(input_shape=(32, 32, 1))\n",
    "dae.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(patience=3)\n",
    "]\n",
    "dae.fit(train_data, train_data, epochs=50, batch_size=32, validation_data=(val_data, val_data), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a0244b0-c6a0-4dd4-aa40-18ff4b63da30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extract latent representations\n",
    "latent_train = dae.predict(train_data)\n",
    "latent_val = dae.predict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b37db3-a9e4-430f-835f-c10fe497210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess test data\n",
    "test_data = load_audio_files(test_data_path)\n",
    "test_data = resize_data(test_data, target_shape=(32, 32))\n",
    "test_data = safe_normalize(test_data)\n",
    "test_data = test_data[..., np.newaxis]\n",
    "latent_test = dae.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "820fda1a-294c-4b2d-8884-deb4b45c458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for GCRNN\n",
    "latent_train = latent_train[:, np.newaxis, :, :, :]\n",
    "latent_val = latent_val[:, np.newaxis, :, :, :]\n",
    "latent_test = latent_test[:, np.newaxis, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b587b5c-c917-4176-ba39-d8290911a70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 107ms/step - accuracy: 0.9192 - loss: 0.1361 - val_accuracy: 1.0000 - val_loss: 0.1768 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 2.0038e-07 - val_accuracy: 1.0000 - val_loss: 0.0837 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 1.7433e-07 - val_accuracy: 1.0000 - val_loss: 0.0388 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 6.9254e-07 - val_accuracy: 1.0000 - val_loss: 0.0162 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 9.5642e-08 - val_accuracy: 1.0000 - val_loss: 0.0063 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 2.9942e-08 - val_accuracy: 1.0000 - val_loss: 0.0021 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 1.4855e-07 - val_accuracy: 1.0000 - val_loss: 6.5128e-04 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 7.5187e-08 - val_accuracy: 1.0000 - val_loss: 1.8234e-04 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 1.1056e-06 - val_accuracy: 1.0000 - val_loss: 4.1629e-05 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 2.1547e-07 - val_accuracy: 1.0000 - val_loss: 8.6119e-06 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 7.4061e-08 - val_accuracy: 1.0000 - val_loss: 1.5797e-06 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 3.1143e-06 - val_accuracy: 1.0000 - val_loss: 2.5932e-07 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 5.9712e-08 - val_accuracy: 1.0000 - val_loss: 4.5346e-08 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 4.2351e-07 - val_accuracy: 1.0000 - val_loss: 7.9431e-09 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 1.6604e-08 - val_accuracy: 1.0000 - val_loss: 1.5849e-09 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 1.7796e-07 - val_accuracy: 1.0000 - val_loss: 3.7405e-10 - learning_rate: 1.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 2.3060e-06 - val_accuracy: 1.0000 - val_loss: 9.1712e-11 - learning_rate: 1.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 1.6675e-07 - val_accuracy: 1.0000 - val_loss: 3.0208e-11 - learning_rate: 1.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 2.4937e-07 - val_accuracy: 1.0000 - val_loss: 1.1126e-11 - learning_rate: 1.0000e-06\n",
      "Epoch 20/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 2.0296e-06 - val_accuracy: 1.0000 - val_loss: 4.8979e-12 - learning_rate: 1.0000e-06\n",
      "Epoch 21/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 6.6487e-08 - val_accuracy: 1.0000 - val_loss: 2.8711e-12 - learning_rate: 1.0000e-06\n",
      "Epoch 22/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 6.0069e-08 - val_accuracy: 1.0000 - val_loss: 1.7775e-12 - learning_rate: 1.0000e-07\n",
      "Epoch 23/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 6.4505e-06 - val_accuracy: 1.0000 - val_loss: 1.3404e-12 - learning_rate: 1.0000e-07\n",
      "Epoch 24/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 2.5499e-07 - val_accuracy: 1.0000 - val_loss: 1.0547e-12 - learning_rate: 1.0000e-07\n",
      "Epoch 25/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 1.8116e-07 - val_accuracy: 1.0000 - val_loss: 8.8732e-13 - learning_rate: 1.0000e-08\n",
      "Epoch 26/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 5.9571e-07 - val_accuracy: 1.0000 - val_loss: 7.7699e-13 - learning_rate: 1.0000e-08\n",
      "Epoch 27/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 1.1431e-05 - val_accuracy: 1.0000 - val_loss: 6.1172e-13 - learning_rate: 1.0000e-08\n",
      "Epoch 28/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 3.0943e-08 - val_accuracy: 1.0000 - val_loss: 6.0966e-13 - learning_rate: 1.0000e-09\n",
      "Epoch 29/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 2.1924e-07 - val_accuracy: 1.0000 - val_loss: 5.6809e-13 - learning_rate: 1.0000e-09\n",
      "Epoch 30/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 4.8782e-08 - val_accuracy: 1.0000 - val_loss: 5.7144e-13 - learning_rate: 1.0000e-09\n",
      "Epoch 31/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 4.8993e-08 - val_accuracy: 1.0000 - val_loss: 5.2490e-13 - learning_rate: 1.0000e-10\n",
      "Epoch 32/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 1.1359e-07 - val_accuracy: 1.0000 - val_loss: 5.3375e-13 - learning_rate: 1.0000e-10\n",
      "Epoch 33/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 1.0721e-05 - val_accuracy: 1.0000 - val_loss: 5.3588e-13 - learning_rate: 1.0000e-10\n",
      "Epoch 34/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 1.1977e-07 - val_accuracy: 1.0000 - val_loss: 5.4405e-13 - learning_rate: 1.0000e-11\n",
      "Epoch 35/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 4.2763e-08 - val_accuracy: 1.0000 - val_loss: 5.4630e-13 - learning_rate: 1.0000e-11\n",
      "Epoch 36/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 1.3206e-07 - val_accuracy: 1.0000 - val_loss: 5.3464e-13 - learning_rate: 1.0000e-11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b22ec35b50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train GCRNN\n",
    "gcrnn = build_gcrnn(input_shape=latent_train.shape[1:])\n",
    "gcrnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gcrnn.fit(latent_train, np.zeros(len(latent_train)), epochs=50, batch_size=32, \n",
    "          validation_data=(latent_val, np.zeros(len(latent_val))), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b796aba-ce88-46c6-9470-76807b19ed21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Validation Set Evaluation:\n",
      "Accuracy: 0.9469964664310954\n",
      "Precision: 0.0\n",
      "Recall: 1.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix:\n",
      " [[268  15]\n",
      " [  0   0]]\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Testing Set Evaluation:\n",
      "Accuracy: 0.9467680608365019\n",
      "Precision: 0.0\n",
      "Recall: 1.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix:\n",
      " [[249  14]\n",
      " [  0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation and test sets\n",
    "for dataset_name, data, labels in [\n",
    "    (\"Validation\", latent_val, np.zeros(len(latent_val))),\n",
    "    (\"Testing\", latent_test, np.zeros(len(latent_test))),\n",
    "]:\n",
    "    predictions = gcrnn.predict(data)\n",
    "    threshold = np.percentile(predictions, 95)\n",
    "    binary_predictions = (predictions > threshold).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(labels, binary_predictions)\n",
    "    precision = precision_score(labels, binary_predictions, average='binary', zero_division=1)\n",
    "    recall = recall_score(labels, binary_predictions, average='binary', zero_division=1)\n",
    "    f1 = f1_score(labels, binary_predictions, average='binary', zero_division=1)\n",
    "    conf_matrix = confusion_matrix(labels, binary_predictions)\n",
    "\n",
    "    print(f\"{dataset_name} Set Evaluation:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9eb2bb-8f46-4d57-9dfe-1ba55e8f3eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
